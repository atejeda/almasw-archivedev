###############################################################################
#
# This is a logstah configuration for processing ALMA logs for IntTest
# parse and rearrange some fields, it tries to use the same fields used
# in the original ALMA kibana deployment, this is an example of the generated
# entry for elastic search:
#
# {
#     "SourceObject" => "AcsContainerRunner",
#           "source" => "/home/user/test/tmp/ACS_INSTANCE.0/all_logs.xml.233",
#             "Data" => "{content=LOG_CompAct_Init_OK, Name=logName}",
#       "@timestamp" => 2019-03-12T19:37:03.587Z,
#           "Thread" => "main",
#             "text" => "AcsContainerRunner#run with arguments -OAIAddr",
#          "Process" => "AcsContainerRunner",
#         "LogLevel" => "Info",
#            "LogId" => "1",
#            "build" => "233",
#             "File" => "AcsContainerRunner.java",
#             "Time" => "2019-03-05T21:43:12.453",
#          "Routine" => "run",
#         "@version" => "1",
#             "Line" => "155",
#             "Host" => "user.machine.hostname"
# }
#
# Some of these fields are not in the xml, hence not parsed.
#
###############################################################################

input {
    beats {
    port => 5044
  }
}

filter {

    # drop unparseable not interesting entries

    if ([message] =~ /<\?xml.*/) {
       drop{}
    }

    if ([message] =~ /<Log.*/) {
      drop{}
    }

    if ([message] =~ /<Header.*/) {
      drop{}
    }

    # convert xml to json and get the log level
    xml {
        source => "message"
        target => doc
        xpath => [
            "name(*)", "LogLevel"
        ]
    }

    # flatten out the log level
    mutate {
        join => {
            "LogLevel" => ""
        }
    }

    # Evict Delouse level, too many logs
    if ([LogLevel] =~ "Delouse") {
        drop{}
    }

    # flatten out the data, if exists
    if ([doc][Data]) {
        mutate {
            add_field => {
                "Data" => "%{doc[Data]}"
            }
            convert => {
                "Data" => "string"
            }
        }
    }

   # flatten out the doc object built in the xml section, add them if exists

   if ([doc][File])         {
      mutate { add_field => { "File"         => "%{doc[File]}"         }}}
   if ([doc][Process])      {
      mutate { add_field => { "Process"      => "%{doc[Process]}"      }}}
   if ([doc][Host])         {
      mutate { add_field => { "Host"         => "%{doc[Host]}"         }}}
   if ([doc][SourceObject]) {
      mutate { add_field => { "SourceObject" => "%{doc[SourceObject]}" }}}
   if ([doc][Thread])       {
      mutate { add_field => { "Thread"       => "%{doc[Thread]}"       }}}
   if ([doc][content])      {
      mutate { add_field => { "text"         => "%{doc[content]}"      }}}
   if ([doc][TimeStamp])    {
      mutate { add_field => { "Time"         => "%{doc[TimeStamp]}"    }}}
   if ([doc][Line])         {
      mutate { add_field => { "Line"         => "%{doc[Line]}"         }}}
   if ([doc][Routine])      {
      mutate { add_field => { "Routine"      => "%{doc[Routine]}"      }}}
   if ([doc][LogId])        {
      mutate { add_field => { "LogId"        => "%{doc[LogId]}"        }}}

    # remove unwanted leading and trailing space from content or cdata
    mutate {
        strip => ["text"]
    }

    # source field is /a/path/all_logs.xml.233, in which 233 is the job id
    # and build the source path again, the all_logs.xml.{id} is a convention
    # for our testing environment
    mutate {
        split => {
            "source" => "."
        }
        add_field => {
            "build" => "%{source[-1]}"
        }
    }

    # join source fields previously splitted elements back to the its original
    # state /a/path/all_logs.xml.233
    mutate {
        join => {
            "source" => "."
        }
    }

    # transform the date to be the same as kibana.alma.cl
    date {
        match => [ "Time", "yyyy-MMM-dd'T'HH:mm:ss.SSS" ]
    }

    # remove unwanted stuff injected by filebeats and logstash
    mutate {
        remove_field => [
            "message",
            "tags",
            "beat",
            "prospector",
            "tags",
            "input",
            "doc",
            "host",
            "offset"
            #"@vesion"    # unclear what this is
            #"@timestamp" # when the file was process by filebeat
      ]
    }
}

output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "logstash-xml-%{+YYYY.MM.dd}"
        ssl => false
    }
    # uncomment this line to debug what's logstash is doing while converting
    # stdout { codec => rubydebug }
}